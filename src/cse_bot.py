import requests
from bs4 import BeautifulSoup
import os
import time
import json
import re
import urllib3
import traceback
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from dotenv import load_dotenv

load_dotenv()

# ===[ì„¤ì • ì˜ì—­]==========================
DISCORD_WEBHOOK_URL = os.environ.get("cse_WEBHOOK_URL")
MONITOR_WEBHOOK_URL = os.environ.get("MONITOR_WEBHOOK_URL")
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_FILE = os.path.join(BASE_DIR, "..", "data", "cse_data.json")

# ê²Œì‹œíŒ ëª©ë¡
TARGET_BOARDS = [
    {
        "id": "bachelor", 
        "name": "í•™ì‚¬ê³µì§€", 
        "url": "https://computer.cnu.ac.kr/computer/notice/bachelor.do?articleLimit=20"
    },
    {
        "id": "general", 
        "name": "êµë‚´ì¼ë°˜ì†Œì‹", 
        "url": "https://computer.cnu.ac.kr/computer/notice/notice.do?articleLimit=20" 
    },
    {
        "id": "job", 
        "name": "êµì™¸í™œë™Â·ì¸í„´Â·ì·¨ì—…", 
        "url": "https://computer.cnu.ac.kr/computer/notice/job.do?articleLimit=20" 
    },
    {
        "id": "project", 
        "name": "ì‚¬ì—…ë‹¨ì†Œì‹", 
        "url": "https://computer.cnu.ac.kr/computer/notice/project.do?articleLimit=20" 
    }
]

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}
# ==========================================


# ===[ì„¸ì…˜ ìƒì„±ê¸°]===
def get_session():
    """ë¹ ë¥¸ ì‹¤íŒ¨ + ì ì ˆí•œ ì¬ì‹œë„"""
    session = requests.Session()
    retry = Retry(
        total=2,  # ì¬ì‹œë„ 2íšŒë¡œ ìµœì†Œí™”
        backoff_factor=1,  # 1ì´ˆ, 2ì´ˆ ëŒ€ê¸°
        status_forcelist=[500, 502, 503, 504],
        raise_on_status=False
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session 


# ===[ID ì¶”ì¶œê¸°]===
def extract_article_id(link):
    """ë§í¬ì—ì„œ articleNo(ê³ ìœ ë²ˆí˜¸) ì¶”ì¶œ"""
    match = re.search(r'articleNo=(\d+)', link)
    if match:
        return int(match.group(1))
    return 0


# ===[ë””ì½” ì „ì†¡ê¸°]===
def send_discord_batch_alert(category_name, new_notices):
    """ë””ìŠ¤ì½”ë“œ ì „ì†¡"""
    if not new_notices:
        return

    if not DISCORD_WEBHOOK_URL:
        send_simple_error_log("ì›¹í›… URLì´ ì—†ìŒ")
        print("âš  ì›¹í›… URLì´ ì—†ìŒ")
        return
    
    count = len(new_notices)
    message_content = f"### ğŸ“¢ [{category_name}] ìƒˆ ê¸€ {count}ê±´\n\n"
    
    for notice in new_notices:
        icon = "â–¶" if notice['is_top'] else "â–·"
        message_content += f"{icon} [{notice['title']}](<{notice['link']}>)\n"
    
    try:
        requests.post(DISCORD_WEBHOOK_URL, json={"content": message_content}, timeout=5)
        print(f"âœ‰ [ì „ì†¡ ì™„ë£Œ] {category_name} - {count}ê±´")
    except Exception as e:
        send_simple_error_log("ê³µì§€ ì „ì†¡ ì‹¤íŒ¨")
        print(f"âš  [ì „ì†¡ ì‹¤íŒ¨] {e}")


# ===[ê´€ë¦¬ì ì•Œë¦¼]===
def send_simple_error_log(message=None):
    """[ê´€ë¦¬ììš©] ì—ëŸ¬ ë°œìƒ ì•Œë¦¼"""
    if not MONITOR_WEBHOOK_URL:
        return 

    now = time.strftime('%Y-%m-%d %H:%M:%S')
    if message:
        content = f"ğŸš¨ **[CSE ë´‡ ì˜¤ë¥˜]** \n{message}\n({now})"
    else:
        content = f"ğŸš¨ **[CSE ë´‡ ì˜¤ë¥˜]** \n{now}"
    
    try:
        requests.post(MONITOR_WEBHOOK_URL, json={"content": content}, timeout=5)
        print("âœ‰ ê´€ë¦¬ì ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ")
    except:
        print("âš  ê´€ë¦¬ì ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨")


# ===[ê²Œì‹œíŒ ê²€ì‚¬]===
def check_board(session, board_info, saved_data):
    """ê°œë³„ ê²Œì‹œíŒ í™•ì¸ ë° ìƒˆ ê¸€ ê°ì§€"""
    board_id = board_info["id"]
    board_name = board_info["name"]
    url = board_info["url"]

    print(f"â— [{board_name}] ë¶„ì„ ì¤‘...")

    try:
        # íƒ€ì„ì•„ì›ƒ: ì—°ê²° 15ì´ˆ, ì½ê¸° 25ì´ˆ (ì•ˆì •ì„±ê³¼ ì†ë„ì˜ ê· í˜•)
        response = session.get(
            url, 
            headers=HEADERS, 
            verify=False, 
            timeout=(15, 25)
        )
        
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        rows = soup.select('table.board-table tbody tr')
        
        if not rows:
            send_simple_error_log(f"{board_name}-ê²Œì‹œê¸€(tr)ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            raise Exception(f"âš  [{board_name}] ê²Œì‹œê¸€(tr)ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
        last_id = saved_data.get(board_id, 0)
        new_notices = []
        max_id = last_id

        for row in rows:
            title_div = row.select_one('.b-title-box > a')
            if not title_div:
                continue 

            title = title_div.get('title') or title_div.text.strip()
            title = title.replace("ìì„¸íˆ ë³´ê¸°", "").strip()
            
            href = title_div.get('href')
            
            if href.startswith('?'):
                base_url = url.split('?')[0]
                link = f"{base_url}{href}"
            else:
                link = href
            
            article_id = extract_article_id(link)
            if article_id == 0:
                continue

            row_classes = row.get('class', [])
            is_top = 'b-top-box' in row_classes

            if article_id > last_id:
                new_notices.append({
                    "id": article_id,
                    "title": title,
                    "link": link,
                    "is_top": is_top
                })
                if article_id > max_id:
                    max_id = article_id

        # ìµœì´ˆ ì‹¤í–‰ ì²˜ë¦¬
        if last_id == 0 and max_id > 0:
            print(f"  â˜ ìµœì´ˆ ì‹¤í–‰ - ê¸°ì¤€ì (ID: {max_id})ë§Œ ì„¤ì •")
            saved_data[board_id] = max_id
            return True
        
        # ìƒˆ ê¸€ ì²˜ë¦¬
        if new_notices:
            new_notices.sort(key=lambda x: x['id'])
            send_discord_batch_alert(board_name, new_notices)
            saved_data[board_id] = max_id
            return True
        
        print(f"  â˜ ìƒˆ ê¸€ ì—†ìŒ")
        return False
        
    except requests.exceptions.Timeout:
        print(f"  â± íƒ€ì„ì•„ì›ƒ (ì„œë²„ ì‘ë‹µ ì§€ì—°)")
        return False
        
    except Exception as e:
        error_msg = f"  âš  ì˜¤ë¥˜: {str(e)[:80]}"
        print(error_msg)
        send_simple_error_log(f"{board_name}-ì ‘ì† ì‹¤íŒ¨")
        return False


# ===[MAIN]===
def run_bot():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ìˆœì°¨ ì²˜ë¦¬ ë°©ì‹"""
    print("\n" + "â”" * 40)
    print(f"ğŸ¤– CSE ê³µì§€ë´‡ ì‹¤í–‰: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    try:
        saved_data = {}

        # íŒŒì¼ ì½ê¸°
        if os.path.exists(DATA_FILE):
            with open(DATA_FILE, "r", encoding="utf-8") as f:
                try:
                    saved_data = json.load(f)
                except:
                    saved_data = {}

        session = get_session()
        any_changes = False
        
        print("â˜ ê²Œì‹œíŒ ìˆœì°¨ í™•ì¸ ì¤‘...\n")
        
        for i, board in enumerate(TARGET_BOARDS, 1):
            if check_board(session, board, saved_data):
                any_changes = True
            
            # ë§ˆì§€ë§‰ ê²Œì‹œíŒì´ ì•„ë‹ˆë©´ 2ì´ˆ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            if i < len(TARGET_BOARDS):
                time.sleep(2)
        
        # ë°ì´í„° ì €ì¥
        print()
        if any_changes:
            with open(DATA_FILE, "w", encoding="utf-8") as f:
                json.dump(saved_data, f, ensure_ascii=False, indent=4)
            print("â˜‘ ë°ì´í„° ì €ì¥ ì™„ë£Œ")
        else:
            print("â˜‘ ë³€ë™ ì‚¬í•­ ì—†ìŒ")

    except Exception as e:
        print(f"\nâš  ì¹˜ëª…ì ì¸ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        send_simple_error_log("í”„ë¡œê·¸ë¨ ê°•ì œ ì¢…ë£Œ")


if __name__ == "__main__":
    run_bot()